import os
import sys
import shutil
import streamlit as st
from omegaconf import OmegaConf
from llama_cpp import Llama

# Ensure the root directory is on the Python path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from backend.my_lib.pdf_manager import PDFManager
from backend.my_lib.retrievers import Retrievers
from backend.my_lib.qa_chains import QAchains
from backend.settings import validate_env_secrets

from helper_gui import question_input_output_ui, display_results_ui, pdf_uploader_ui, select_model_ui, get_openai_key

# logging from backend
import logging

logger = logging.getLogger(__name__)


def initialize_session_state() -> None:
    """
    Initialize necessary session state variables for Streamlit.
    """
    # Set 'debug' based on env var, but store it in session_state immediately
    st.session_state.setdefault(
        "debug", os.getenv("DEBUG_MODE", "false").lower() == "true"
    )
    st.session_state.setdefault("pdf_manager", None)
    st.session_state.setdefault("retrievers", None)
    st.session_state.setdefault("qa_chains", None)
    st.session_state.setdefault("answer", "")
    st.session_state.setdefault("qa_history", [])
    logger.debug("Session state initialized.")


# Cache this resource so it's only loaded once per session
@st.cache_resource
def load_local_llama(repo_id: str, filename: str) -> Llama:
    return Llama.from_pretrained(
        repo_id=repo_id,
        filename=filename,
        verbose=False
    )

@st.cache_resource
def vector_store_builder(
    pdf_path: str, _config: OmegaConf, uploaded: list | None
) -> tuple[PDFManager, Retrievers]:
    """
    Process the uploaded PDF documents: load, chunk, and create a vector store.

    Args:
        pdf_path (str): Path to the folder containing PDF files.
        config (OmegaConf): Configuration object.
    """

    logger.info("Building vector store for PDFs at path: %s", pdf_path)

    # Step 1: Load and chunk
    pdf_manager = PDFManager(pdf_path, _config)
    pdf_manager.load_pdfs()
    pdf_manager.chunk_documents()

    # Step 2: Create vector store
    pdf_manager.create_vectorstore()

    # Step 3: Create retrievers
    retrievers = Retrievers(pdf_manager, _config)
    retrievers.setup_retrievers()

    logger.info("Vector store and retrievers created successfully.")
    return pdf_manager, retrievers


def main() -> None:
    """
    Entry point for the Streamlit application that drives the Two-Stage RAG PDF QA system.

    This function orchestrates the entire UI and backend workflow:
    1. Displays a header image and title/subtitle for the app.
    2. Loads configuration settings from the OmegaConf YAML file.
    3. Initializes all required Streamlit session state variables.
    4. Handles PDF upload and triggers vector store creation:
       - If PDFs are provided, invokes `vector_store_builder` to load, chunk, and index documents.
       - Stores the resulting PDFManager and Retrievers objects in session state.
       - Instantiates the QAchains object for downstream question answering.
    5. Renders the question‐input section once retrievers exist:
       - Passes the QAchains instance into `question_input_output_ui`.
       - Captures and stores the user’s question and generated answer in session state.
    6. Displays the latest answer and the Q&A history via `display_results_ui`.

    Session State Keys Used:
        - debug (bool): Toggles debug messages if True.
        - pdf_manager (PDFManager | None): Manages PDF loading and chunking.
        - retrievers (Retrievers | None): Encapsulates BM25 and semantic retrievers.
        - qa_chains (QAchains | None): Orchestrates question shortening, retrieval, and answer generation.
        - answer (str): The most recent answer generated by the pipeline.
        - qa_history (list[tuple[str, str]]): A chronological list of (question, answer) pairs.

    Returns:
        None
    """

    logger.info("Starting Streamlit app")

    # Display the image at the top of the app
    st.image("frontend/static/image.jpeg", use_container_width=True)

    # Load configuration using OmegaConf
    config = OmegaConf.load("configs/config.yaml")
    logger.info("Configuration loaded successfully.")
    # print(config)


    # ==============================
    # Constructing the Layout
    # ==============================
    st.title("Two-Stage RAG System for PDF Question Answering")
    # st.subheader("Fast yet Precise Document Retrieval and Question Answering")
    st.write(
        "Start by **selecting a model** (OpenAI or local LLaMA), then **upload your PDF files**, and finally **ask questions** to extract insights using the two-stage retrieval system."
    ) 
   
    # sidebar    
    st.sidebar.header("App Description")
    st.sidebar.write(
        "This application leverages a two-stage retrieval-augmented generation (RAG) pipeline to efficiently process and extract information from PDF documents. "
        "Users can upload a collection of PDFs and interactively ask questions to receive precise answers based on the document content. "
        "The system supports both OpenAI and local LLaMA models, providing flexibility in model selection and deployment."
    )

    # Initialize session state variables
    initialize_session_state()
    logger.info("Session state initialized successfully.")

    # clear the vector store
    print("vector_store_cleared:", st.session_state.get("vector_store_cleared", False))
    if (
        not st.session_state.get("vector_store_cleared", False)
        and config.Vectorstore.clear_existing
    ):
        shutil.rmtree(config.Vectorstore.persist_directory, ignore_errors=True)
        # rebuild the vector store
        st.session_state.vector_store_cleared = True

    # Check debug mode
    if st.session_state.debug:
        st.warning("DEBUG MODE is ON")
        logger.debug("Debug mode is enabled.")

    # Validate environment secrets
    if not st.session_state.get("env_validated"):
        validate_env_secrets()
        st.session_state.env_validated = True
        logger.info("Environment secrets validated successfully.")
        # st.success("Environment secrets validated successfully!")

    # ==============================
    # Model Selection
    # ==============================
    model_choice = select_model_ui()
    # from helper_gui import get_openai_key
    # Instantiate the chosen LLM
    if model_choice == "OpenAI":
        openai_key = get_openai_key()
        if not openai_key:
            st.warning("Please enter a valid OpenAI API key before proceeding. Key will be stored in your environment—just for this session.")
            st.stop()
        from langchain_openai import ChatOpenAI        
        llm = ChatOpenAI(
            api_key=openai_key,
            model=config.llm.openai_modelID,
            temperature=0.0
        )

        st.session_state['api_key'] = openai_key
        st.info("OpenAI API key loaded successfully end with " + openai_key[-5:])        
    else:  # Local LLaMA
        # display_loading_local_model()
        # adjust these paths or load from config.yaml if you like
        repo_model = config.llm.local_llama_model
        filename   = config.llm.local_llama_filename
        llm = load_local_llama(repo_model, filename)
        st.info("Local LLaMA model loaded successfully.")
    

    # ==============================
    # PDF Upload and vector store creation
    # ==============================
    # pdf_path = pdf_uploader_ui()
    uploaded, pdf_path = pdf_uploader_ui()
    if uploaded is not None:
        # print(uploaded)
        # pdf_path = save_uploaded_pdfs(uploaded, "data/uploads")
        # else:
        # pdf_path = None

        # Create vector store and retrievers only if the pdfs are uploaded successfully
        # if pdf_path is not None:
        logger.info("PDF path provided: %s", pdf_path)
        if st.session_state.debug:
            st.write("pdfs path:", pdf_path)
        # if st.session_state.get("retrievers") is not None:
        #     st.info(
        #         "Vector store is already built - you can proceed to ask your question"
        #     )
        #     logger.info("Vector store already built, skipping creation.")
        pdf_manager, retrievers = vector_store_builder(pdf_path, config, uploaded)
        st.session_state.pdf_manager = pdf_manager
        st.session_state.retrievers = retrievers

        # Create QA chains
        st.session_state.qa_chains = QAchains(retrievers, config, llm)
        st.success("PDFs and vector store processed successfully!")

    # ==============================
    # Question Section (only if retriever is successfully created)
    # ==============================
    if st.session_state.get("retrievers") is not None:
        question, answer = question_input_output_ui(st.session_state.qa_chains)

        if answer is not None:
            st.session_state.answer = answer
            st.session_state.qa_history.append((question, answer))
            logger.info("Question answered: %s, answer: %s", question, answer)

    # ==============================
    # Display answer & history
    # ==============================
    display_results_ui(
        answer=st.session_state.answer,
        qa_history=st.session_state.qa_history,
    )
    logger.info("Displayed results and history.")


if __name__ == "__main__":
    main()
