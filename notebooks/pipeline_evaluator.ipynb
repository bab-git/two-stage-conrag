{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG Pipelines for Document-Based Question Answering\n",
    "\n",
    "This notebook purpose is to build and inspect the parts of the **Two-Stage Consecutive RAG** pipeline.  \n",
    "We use LangChain's `QAGenerateChain` to generate a set of question-answer (QA) pairs from the collection of PDF documents. This set will be used to evaluate the performance of the RAG pipeline.  \n",
    "The performance of the Two-Stage RAG pipeline will be also compared to two other RAG pipelines—**Standard RAG** and **Hybrid RAG** for document-based QA. We compare the accuracy rates of each pipeline to determine their effectiveness in delivering precise and contextually relevant responses based on the provided documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "\n",
      " ======== Configuration parameters:\n",
      "settings:\n",
      "  verbose: false\n",
      "splitter:\n",
      "  large_chunk_size: 2000\n",
      "  small_chunk_size: 400\n",
      "  paragraph_separator: \"\\n \\n\"\n",
      "llm:\n",
      "  openai_modelID: gpt-4o-mini\n",
      "  embed_model_id: sentence-transformers/all-MiniLM-L12-v2\n",
      "  local_llama_model: hugging-quants/Llama-3.2-1B-Instruct-Q4_K_M-GGUF\n",
      "  local_llama_filename: '*q4_k_m.gguf'\n",
      "Vectorstore:\n",
      "  collection_name: large_chunks\n",
      "  persist_directory: vector_store\n",
      "  clear_existing: false\n",
      "Retrieval:\n",
      "  semantic_CE_model: cross-encoder/stsb-TinyBERT-L-4\n",
      "  keyword_CE_model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "  top_k_BM25: 200\n",
      "  top_k_documents: 5\n",
      "  top_k_semantic: 50\n",
      "  top_k_final: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Required imports\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.evaluation.qa import QAGenerateChain, QAEvalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import random\n",
    "# initialize(config_path=\"../configs\", job_name=\"notebook_config\")\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "# Assuming you're in notebooks/ directory\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# from backend.settings import get_env_secrets  # now works!\n",
    "from backend.my_lib.pdf_manager import PDFManager\n",
    "from backend.my_lib.retrievers import Retrievers\n",
    "from backend.my_lib.qa_chains import QAchains\n",
    "from backend.my_lib.hybrid_retrieval import Hybrid_Retrieval\n",
    "\n",
    "config = OmegaConf.load(\"../configs/config.yaml\")\n",
    "print('\\n ======== Configuration parameters:')\n",
    "print(OmegaConf.to_yaml(config))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default question:\n",
      "  According to the documents, what is Morningstar's view on the Federal Reserve's interest rate decisions for the remainder of 2024 and into 2025?\n"
     ]
    }
   ],
   "source": [
    "local_model = True\n",
    "skip_qa_generation = True\n",
    "data_root = \"../data/sample_pdfs/\" # sample pdfs from the repository\n",
    "# data_root = \"../data/pdfs_selected/\" # your uploaded pdfs\n",
    "\n",
    "# config = compose(config_name=\"config\")\n",
    "modelID = config.llm.openai_modelID\n",
    "top_k_BM25 = config.Retrieval.top_k_BM25\n",
    "top_k_semantic = config.Retrieval.top_k_semantic\n",
    "top_k_final = config.Retrieval.top_k_final\n",
    "\n",
    "question = \" According to the documents, what is Morningstar's view on the Federal Reserve's interest rate decisions for the remainder of 2024 and into 2025?\"\n",
    "print('Default question:\\n',question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample QA test case generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-level chunks\n",
    "For the QA sample generation it is sufficient to create only one chunk per pdf file.  \n",
    "Later, we will perform separate finer-grained chunking for the RAG pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/sample_pdfs/Carl Vine_ Japan Holds the ‘Most Fascinating Little Pocket of the Global Equity Market’ _ Morningstar.pdf\n",
      "../data/sample_pdfs/5 Hot Stocks to Sell Before They Report Earnings _ Morningstar.pdf\n",
      "../data/sample_pdfs/4 Risky Stocks to Sell and 4 Picks to Buy Instead _ Morningstar.pdf\n",
      "../data/sample_pdfs/5 Undervalued Stocks to Buy as Their Stories Play Out _ Morningstar.pdf\n",
      "../data/sample_pdfs/5 Undervalued Stocks to Buy During Q4 _ Morningstar.pdf\n",
      "../data/sample_pdfs/August 2024 Stock Market Outlook_ Small-Cap and Value Stocks Shine _ Morningstar.pdf\n",
      "../data/sample_pdfs/5 Stocks to Buy as the Market Rally Broadens _ Morningstar.pdf\n",
      "7 PDF chunks created with one chunk per PDF\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filenames = [f for f in glob.glob(data_root + '*.pdf') if os.path.isfile(f)]\n",
    "\n",
    "documents = []\n",
    "for file in filenames:    \n",
    "    loader = PyPDFLoader(f'{file}')\n",
    "    pages = loader.load()\n",
    "    # add all pages together    \n",
    "    combined_page_content = \"\".join([page.page_content for page in pages])\n",
    "    document = {\"page_content\": combined_page_content, \"metadata\": {\"source\": file}}\n",
    "    documents.append(document)\n",
    "    print(file)\n",
    "print(f'{len(documents)} PDF chunks created with one chunk per PDF\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA pair generation using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bobhosseini/Portfolio/two-stage-conrag/.venv/lib/python3.12/site-packages/langchain/chains/llm.py:370: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "2025-06-19 20:59:56,994 INFO [httpx] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-19 21:00:00,329 INFO [httpx] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 QA pairs generated from PDFs\n",
      "\n",
      "\n",
      " Source file:\n",
      " ../data/sample_pdfs/August 2024 Stock Market Outlook_ Small-Cap and Value Stocks Shine _ Morningstar.pdf\n",
      "Question:\n",
      "('What are the key factors contributing to the recent outperformance of '\n",
      " 'small-cap and value stocks as discussed in the August 2024 Stock Market '\n",
      " 'Outlook by Morningstar?')\n",
      "Answer:\n",
      "('The recent outperformance of small-cap and value stocks is attributed to '\n",
      " 'several key factors: ')\n",
      "\n",
      " Source file:\n",
      " ../data/sample_pdfs/4 Risky Stocks to Sell and 4 Picks to Buy Instead _ Morningstar.pdf\n",
      "Question:\n",
      "(\"What are the key factors influencing Morningstar's recommendations for \"\n",
      " 'selling certain consumer defensive stocks, specifically Walmart and Costco, '\n",
      " 'and what alternative stocks does Morningstar suggest for investors looking '\n",
      " 'for exposure in this sector?')\n",
      "Answer:\n",
      "('Morningstar recommends selling Walmart and Costco due to their significant '\n",
      " 'overvaluation, with Walmart trading at 33 times projected earnings for 2024 '\n",
      " 'and Costco at 50 times. Despite both companies performing well '\n",
      " 'fundamentally, their high valuations do not justify their stock prices in '\n",
      " 'the long term. Instead, Morningstar suggests buying Dollar General and '\n",
      " 'Constellation Brands. Dollar General is rated 5 stars and trades at a 37% '\n",
      " 'discount to fair value, benefiting from its large store network and expected '\n",
      " 'recovery in consumer spending among lower-income households. Constellation '\n",
      " 'Brands is rated 4 stars, trades at a 17% discount, and has shown solid '\n",
      " 'performance in its beer segment, despite some declines in its wine and '\n",
      " 'spirits divisions.')\n"
     ]
    }
   ],
   "source": [
    "# generating n_samples of QA pairs from available PDFs\n",
    "n_samples = 2\n",
    "\n",
    "index = np.random.randint(1, len(documents), n_samples)\n",
    "sample_docs = [documents[i] for i in index]   \n",
    "llm = ChatOpenAI(temperature = 0.0, model=modelID)\n",
    "\n",
    "example_gen_chain = QAGenerateChain.from_llm(llm)\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in sample_docs],\n",
    ")\n",
    "print(f'{len(new_examples)} QA pairs generated from PDFs\\n')\n",
    "\n",
    "for i, example in enumerate(new_examples):\n",
    "    source = sample_docs[i]['metadata']['source']\n",
    "    print(\"\\n Source file:\\n\", source)    \n",
    "    example['source'] = source\n",
    "    print(f'Question:')\n",
    "    pprint(f'{example.get('qa_pairs').get('query')}')\n",
    "    print('Answer:')\n",
    "    pprint(f'{example.get('qa_pairs').get('answer')}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Stage RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 23:23:22.842 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-19 23:23:22,842 INFO [backend.my_lib.pdf_manager] Total document pages loaded: 134 from ../data/sample_pdfs/\n",
      "2025-06-19 23:23:22.848 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-19 23:23:22,848 INFO [backend.my_lib.pdf_manager] Documents split into 758 small and 215 large chunks.\n",
      "2025-06-19 23:23:22,849 INFO [sentence_transformers.SentenceTransformer] Use pytorch device_name: mps\n",
      "2025-06-19 23:23:22,850 INFO [sentence_transformers.SentenceTransformer] Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L12-v2\n",
      "2025-06-19 23:23:24,442 INFO [backend.my_lib.pdf_manager] Collection large_chunks is deleted\n",
      "2025-06-19 23:23:25.013 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-19 23:23:25,016 INFO [backend.my_lib.pdf_manager] Vectorstore large_chunks created successfully with 215 documents.\n",
      "2025-06-19 23:23:26,136 INFO [sentence_transformers.cross_encoder.CrossEncoder] Use pytorch device: mps\n",
      "2025-06-19 23:23:27,591 INFO [sentence_transformers.cross_encoder.CrossEncoder] Use pytorch device: mps\n",
      "2025-06-19 23:23:27.944 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-19 23:23:27,946 INFO [backend.my_lib.retrievers] Retrievers created successfully.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# config.settings.verbose = True\n",
    "\n",
    "pdf_manager = PDFManager(data_root, config)\n",
    "pdf_manager.load_pdfs()\n",
    "pdf_manager.chunk_documents()\n",
    "pdf_manager.create_vectorstore()\n",
    "\n",
    "# Create retrievers\n",
    "retrievers = Retrievers(pdf_manager, config)\n",
    "retrievers.setup_retrievers()\n",
    "\n",
    "# create qa chains\n",
    "qa_chains = QAchains(retrievers, config)\n",
    "\n",
    "def two_stage_rag(question):\n",
    "    qa_chains.shorten_question(question)\n",
    "    qa_chains.retrieve_context()\n",
    "    answer = qa_chains.generate_answer()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection of the two-stage RAG with a sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"What are the expectations for the Federal Reserve's interest rate cuts \"\n",
      " 'according to David Sekera, and how do these expectations relate to the '\n",
      " 'upcoming Fed meetings and inflation data?')\n"
     ]
    }
   ],
   "source": [
    "question = new_examples[0]['qa_pairs']['query']\n",
    "pprint(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federal Reserve, interest rate cuts, David Sekera, expectations, Fed meetings, inflation data.\n"
     ]
    }
   ],
   "source": [
    "# Question shortening\n",
    "qa_chains.shorten_question(question)\n",
    "shortened_question = qa_chains.shortened_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The small chunks were retrieved\n",
      "The DRS was calculated for relevant PDF documents\n",
      "\n",
      " ==== Ranked retrieved large chunks ==== \n",
      "\n",
      "The large chunks were retrieved\n",
      "The aggregated scores were calculated for all retrieved chunks\n",
      "The top score chunks were concatenated\n",
      "Number of top score chunks retrieved: 20\n",
      "\n",
      "====== Content of a retrieved top score chunk:\n",
      " And of course, that’s still our base case scenario as far as what we expect the Fed to\n",
      "start cutting interest rates.\n",
      "Dziubinski: So then what are you going to be listening for during Fed Chair Powell’s\n",
      "remarks? And from the Fed statement?\n",
      "Sekera: I think this is going to be actually a pretty boring, uneventful Fed meeting. I\n",
      "\n",
      "====== The chunk metadata:\n",
      " {'name': '5 Undervalued Stocks to Buy as Their Stories Play Out _ Morningstar.pdf', 'page': 2, 'score': 1.208620309829712, 'aggregated_score': np.float64(1.0374444365488267)}\n"
     ]
    }
   ],
   "source": [
    "# Test the retrievers\n",
    "qa_chains.retrieve_context()\n",
    "top_score_docs = qa_chains.top_score_docs\n",
    "i_chunk = random.randint(0, len(top_score_docs)-1)\n",
    "print('Number of top score chunks retrieved:',len(top_score_docs))\n",
    "print('\\n====== Content of a retrieved top score chunk:\\n',top_score_docs[i_chunk].page_content)\n",
    "print(f'\\n====== The chunk metadata:\\n {top_score_docs[i_chunk].metadata}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('According to David Sekera, the Morningstar US economics team is expecting a '\n",
      " '25-basis-point cut in the federal-funds rate for each of the next two '\n",
      " 'meetings. They anticipate that the federal-funds rate will decline to a '\n",
      " 'range of 3.00% to 3.25% by the end of next year. However, Sekera notes a '\n",
      " 'caveat: if employment remains strong, there is a possibility that the Fed '\n",
      " 'may not cut the rate as much as expected.\\n'\n",
      " '\\n'\n",
      " 'In terms of the upcoming Fed meetings, Sekera indicates that the market '\n",
      " 'currently implies a 0% chance of a rate cut at the next meeting, with only a '\n",
      " '10% chance for the July meeting. The probability increases to 50% for the '\n",
      " 'September meeting, which aligns with their base case scenario for when the '\n",
      " 'Fed might begin cutting rates.\\n'\n",
      " '\\n'\n",
      " 'Regarding inflation data, Sekera suggests that if inflation numbers come out '\n",
      " 'higher than expected, it could lead to a sharp market selloff and would call '\n",
      " \"into question the Fed's ability to manage inflation effectively. He believes \"\n",
      " 'that the inflation numbers should be under control, but any surprises could '\n",
      " \"impact the Fed's decision-making process regarding interest rate cuts.\")\n"
     ]
    }
   ],
   "source": [
    "qa_chains.generate_answer()\n",
    "answer = qa_chains.response\n",
    "pprint(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Stage RAG evaluation using generated QA pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1\n",
      "('What are the key economic indicators and earnings reports that David Sekera '\n",
      " 'is focusing on in the week of July 15, 2024, and what implications do they '\n",
      " 'have for consumer spending and stock valuations?')\n",
      "- Key economic indicators  \n",
      "- Earnings reports  \n",
      "- David Sekera  \n",
      "- Week of July 15, 2024  \n",
      "- Implications for consumer spending  \n",
      "- Stock valuations  \n",
      "The shortened question:\n",
      " - Key economic indicators  \n",
      "- Earnings reports  \n",
      "- David Sekera  \n",
      "- Week of July 15, 2024  \n",
      "- Implications for consumer spending  \n",
      "- Stock valuations  \n",
      "The small chunks were retrieved\n",
      "The DRS was calculated for relevant PDF documents\n",
      "\n",
      " ==== Ranked retrieved large chunks ==== \n",
      "\n",
      "The large chunks were retrieved\n",
      "The aggregated scores were calculated for all retrieved chunks\n",
      "The top score chunks were concatenated\n",
      "Document 4\n",
      "(\"What are the key factors influencing Morningstar's stock recommendations for \"\n",
      " 'Q4 2024, particularly regarding the economic outlook, interest rates, and '\n",
      " 'specific stock picks?')\n",
      "Morningstar, stock recommendations, Q4 2024, economic outlook, interest rates, stock picks.\n",
      "The shortened question:\n",
      " Morningstar, stock recommendations, Q4 2024, economic outlook, interest rates, stock picks.\n",
      "The small chunks were retrieved\n",
      "The DRS was calculated for relevant PDF documents\n",
      "\n",
      " ==== Ranked retrieved large chunks ==== \n",
      "\n",
      "The large chunks were retrieved\n",
      "The aggregated scores were calculated for all retrieved chunks\n",
      "The top score chunks were concatenated\n",
      "The result of RAG evaluation for the given example questions: \n",
      "\n",
      " Accuracy of the RAG pipeline: 0.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'results': 'GRADE: INCORRECT'}, {'results': 'INCORRECT'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = list()\n",
    "examples = list()\n",
    "for example, idoc in zip(new_examples, index):\n",
    "    print(f'Document {idoc}')\n",
    "    question = example['qa_pairs']['query']\n",
    "    pprint(question)\n",
    "    answer = two_stage_rag(question)\n",
    "    prediction = {'query': example['qa_pairs']['query'], 'answer': example['qa_pairs']['answer']}\n",
    "    prediction[\"result\"] = answer\n",
    "    predictions.append(prediction)\n",
    "    examples.append(\n",
    "            example.get('qa_pairs')\n",
    "        )\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs_2RAG = eval_chain.evaluate(examples, predictions, prediction_key=\"result\")\n",
    "print('The result of RAG evaluation for the given example questions: ')\n",
    "\n",
    "accuracy_2RAG = len([result for result in graded_outputs_2RAG if result['results'] == 'CORRECT'])/len(graded_outputs_2RAG)\n",
    "print(f'\\n Accuracy of the RAG pipeline: {accuracy_2RAG} \\n')\n",
    "\n",
    "graded_outputs_2RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to Hybrid RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retrieval = Hybrid_Retrieval(pdf_manager, retrievers, config)\n",
    "hybrid_RAG_QA = QAchains(retrievers, config)\n",
    "def hybrid_rag(question,top_k_BM25, top_k_semantic, top_k_final, rrf_k = 60, hybrid = True):\n",
    "    top_score_docs = hybrid_retrieval.hybrid_retriever(question, top_k_BM25, top_k_semantic, top_k_final, rrf_k, hybrid)\n",
    "    hybrid_RAG_QA.top_score_docs = top_score_docs\n",
    "    hybrid_RAG_QA.question = question\n",
    "    answer = hybrid_RAG_QA.generate_answer()\n",
    "    return answer\n",
    "# answer_hybrid = hybrid_rag(question, 200, 50, 10)    \n",
    "# pprint(answer_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA pair evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "('What are the key economic indicators and earnings reports that David Sekera '\n",
      " 'is focusing on in the week of July 15, 2024, and what implications do they '\n",
      " 'have for consumer spending and stock valuations?')\n",
      "=== Hybrid Retrieval with BM25 and semantic search ===\n",
      "Answer:\n",
      "('In the week of July 15, 2024, David Sekera is focusing on several key '\n",
      " 'economic indicators and earnings reports, particularly retail sales and the '\n",
      " 'beginning of earnings season. \\n'\n",
      " '\\n'\n",
      " '1. **Retail Sales**: Sekera is particularly interested in the retail sales '\n",
      " 'data, as it will provide insights into consumer spending patterns. He notes '\n",
      " 'that low-income consumers have been under pressure from inflation for over a '\n",
      " 'year, and there are emerging signs that middle-income households are also '\n",
      " 'beginning to pull back on spending, especially in discretionary and '\n",
      " 'indulgent categories. This trend could indicate a broader slowdown in '\n",
      " 'consumer spending, which is critical for evaluating the health of the '\n",
      " 'economy and the performance of retail stocks.\\n'\n",
      " '\\n'\n",
      " '2. **Earnings Reports**: The start of earnings season is another focal '\n",
      " 'point. Sekera will be analyzing how companies report their earnings, '\n",
      " 'especially in light of the pressures on consumer spending. He mentions that '\n",
      " 'traders are currently focused on near-term profit pressures, which could '\n",
      " 'affect stock valuations.\\n'\n",
      " '\\n'\n",
      " 'The implications of these indicators are significant:\\n'\n",
      " '- A decline in retail sales could suggest that consumer confidence is '\n",
      " 'waning, leading to further reductions in discretionary spending. This would '\n",
      " 'likely impact the earnings of consumer-focused companies and could lead to '\n",
      " 'downward adjustments in stock valuations.\\n'\n",
      " '- Conversely, if retail sales show resilience, it could bolster confidence '\n",
      " 'in consumer spending and support stock prices, particularly for companies in '\n",
      " 'the retail sector.\\n'\n",
      " '\\n'\n",
      " \"Overall, Sekera's analysis indicates that the retail sales data and earnings \"\n",
      " 'reports will be crucial in shaping market expectations and influencing stock '\n",
      " 'valuations in the near term.')\n",
      "Question:\n",
      "(\"What are the key factors influencing Morningstar's stock recommendations for \"\n",
      " 'Q4 2024, particularly regarding the economic outlook, interest rates, and '\n",
      " 'specific stock picks?')\n",
      "=== Hybrid Retrieval with BM25 and semantic search ===\n",
      "Answer:\n",
      "(\"Morningstar's stock recommendations for Q4 2024 are influenced by several \"\n",
      " 'key factors:\\n'\n",
      " '\\n'\n",
      " '1. **Economic Outlook**: The US Economics team at Morningstar anticipates a '\n",
      " 'combination of moderating inflation and slowing economic growth. This '\n",
      " 'backdrop is expected to lead the Federal Reserve to begin cutting the '\n",
      " 'federal-funds rate, with a projected range of 4.75% to 5.00% by the end of '\n",
      " '2024. The team also forecasts that long-term interest rates will decline, '\n",
      " 'with the 10-year US Treasury yield averaging 4.25% in 2024 and 3.50% in '\n",
      " '2025.\\n'\n",
      " '\\n'\n",
      " '2. **Interest Rates**: The expectation of interest rate cuts is significant '\n",
      " 'for stock market performance. The market-implied probability of a rate cut '\n",
      " 'in September 2024 was noted to be over 90%, which supports the case for a '\n",
      " 'more favorable investment environment. This anticipated easing of monetary '\n",
      " 'policy is expected to impact net interest margins positively for banks and '\n",
      " 'influence overall market volatility.\\n'\n",
      " '\\n'\n",
      " '3. **Stock Picks**: Morningstar has identified several undervalued stocks to '\n",
      " 'buy during Q4 2024. While specific stock names are mentioned in the context, '\n",
      " 'the document highlights that investors should consider positioning '\n",
      " 'themselves at a market weight within their long-term asset allocations '\n",
      " 'between equity and fixed income. The focus on stocks that are currently '\n",
      " 'undervalued suggests a strategy to capitalize on potential market '\n",
      " 'pullbacks.\\n'\n",
      " '\\n'\n",
      " 'Overall, the combination of a slowing economy, expected interest rate cuts, '\n",
      " \"and a focus on undervalued stocks shapes Morningstar's recommendations for \"\n",
      " 'investors in Q4 2024.')\n",
      "The result of RAG evaluation for the given example questions: \n",
      "\n",
      " Accuracy of the RAG pipeline: 0.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'results': 'GRADE: CORRECT'}, {'results': 'INCORRECT'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_hybrid = list()\n",
    "examples = list()\n",
    "for example, idoc in zip(new_examples, index):\n",
    "    question = example['qa_pairs']['query']\n",
    "    print(f'Question:')\n",
    "    pprint(question)\n",
    "    answer = hybrid_rag(question, top_k_semantic, top_k_semantic, top_k_final)\n",
    "    print(f'Answer:')\n",
    "    pprint(answer)\n",
    "    prediction = {'query': example['qa_pairs']['query'], 'answer': example['qa_pairs']['answer']}\n",
    "    prediction[\"result\"] = answer\n",
    "    predictions_hybrid.append(prediction)\n",
    "    examples.append(\n",
    "            example.get('qa_pairs')\n",
    "        )\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs_hybrid = eval_chain.evaluate(examples, predictions_hybrid, prediction_key=\"result\")\n",
    "print('The result of RAG evaluation for the given example questions: ')\n",
    "\n",
    "accuracy_hybrid = len([result for result in graded_outputs_hybrid if result['results'] == 'CORRECT'])/len(graded_outputs_hybrid)\n",
    "print(f'\\n Accuracy of the RAG pipeline: {accuracy_hybrid} \\n')\n",
    "\n",
    "graded_outputs_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to Normal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "('What are the key economic indicators and earnings reports that David Sekera '\n",
      " 'is focusing on in the week of July 15, 2024, and what implications do they '\n",
      " 'have for consumer spending and stock valuations?')\n",
      "=== Semantic search retrieval only === \n",
      "Answer:\n",
      "('In the week of July 15, 2024, David Sekera is focusing on several key '\n",
      " 'economic indicators and earnings reports, particularly the Consumer Price '\n",
      " 'Index (CPI) and Producer Price Index (PPI) numbers. The CPI and core CPI '\n",
      " 'came in well below expectations, which is a positive sign for inflation '\n",
      " 'control. Conversely, the PPI was slightly higher than expected, but the '\n",
      " 'underlying components were lower than anticipated, suggesting that inflation '\n",
      " 'pressures may not be as severe as previously thought.\\n'\n",
      " '\\n'\n",
      " 'These indicators have significant implications for consumer spending. Sekera '\n",
      " 'notes that low-income consumers have been under pressure from inflation for '\n",
      " 'over a year, and there are signs that middle-income households are also '\n",
      " 'beginning to pull back on discretionary spending. This trend could impact '\n",
      " 'retail sales, which Sekera is closely monitoring.\\n'\n",
      " '\\n'\n",
      " 'Regarding stock valuations, the expectation of a potential interest rate cut '\n",
      " 'in September—now with a market-implied probability of over 90%—could bolster '\n",
      " 'stock prices. If the Fed begins to cut rates, it may lead to improved market '\n",
      " 'conditions and potentially higher stock valuations, especially for companies '\n",
      " 'that are currently undervalued or facing idiosyncratic risks. Sekera '\n",
      " 'emphasizes the need for investors to conduct thorough research and due '\n",
      " 'diligence on individual stocks, particularly those with unique risks, to '\n",
      " 'develop a solid investment thesis.')\n",
      "Question:\n",
      "(\"What are the key factors influencing Morningstar's stock recommendations for \"\n",
      " 'Q4 2024, particularly regarding the economic outlook, interest rates, and '\n",
      " 'specific stock picks?')\n",
      "=== Semantic search retrieval only === \n",
      "Answer:\n",
      "(\"Morningstar's stock recommendations for Q4 2024 are influenced by several \"\n",
      " 'key factors:\\n'\n",
      " '\\n'\n",
      " '1. **Economic Outlook**: The economic environment is characterized by '\n",
      " 'expectations of easing monetary policy, particularly following the release '\n",
      " 'of the June Consumer Price Index (CPI) report, which showed inflation well '\n",
      " 'below expectations. This has led to a market rotation into small-cap and '\n",
      " 'value stocks, as historically, these stocks tend to outperform in a falling '\n",
      " 'interest rate environment.\\n'\n",
      " '\\n'\n",
      " \"2. **Interest Rates**: Morningstar's economic team is anticipating a cut in \"\n",
      " \"interest rates at the Federal Reserve's September meeting, with \"\n",
      " 'market-implied probabilities of a cut exceeding 90%. This expectation is '\n",
      " 'bolstered by recent inflation data, which suggests that the Fed may have the '\n",
      " 'leeway to ease monetary policy.\\n'\n",
      " '\\n'\n",
      " '3. **Stock Picks**: Specific stock recommendations for Q4 include companies '\n",
      " 'like Alphabet Inc. (GOOGL), Verizon Communications (VZ), Nike Inc. Class B '\n",
      " '(NKE), Dow Inc. (DOW), and FMC Corp (FMC). These selections are likely based '\n",
      " 'on their perceived undervaluation and potential for growth in the current '\n",
      " 'economic climate.\\n'\n",
      " '\\n'\n",
      " 'Overall, the combination of a favorable economic outlook, anticipated '\n",
      " 'interest rate cuts, and targeted stock picks in undervalued sectors '\n",
      " \"underpins Morningstar's recommendations for Q4 2024.\")\n",
      "The result of RAG evaluation for the given example questions: \n",
      "\n",
      " Accuracy of the RAG pipeline: 0.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'results': 'GRADE: INCORRECT'}, {'results': 'GRADE: INCORRECT'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_rag = list()\n",
    "examples = list()\n",
    "for example, idoc in zip(new_examples, index):\n",
    "    question = example['qa_pairs']['query']\n",
    "    print(f'Question:')\n",
    "    pprint(question)\n",
    "    answer = hybrid_rag(question, top_k_semantic, top_k_semantic, top_k_final, hybrid = False)\n",
    "    print(f'Answer:')\n",
    "    pprint(answer)\n",
    "    prediction = {'query': example['qa_pairs']['query'], 'answer': example['qa_pairs']['answer']}\n",
    "    prediction[\"result\"] = answer\n",
    "    predictions_rag.append(prediction)\n",
    "    examples.append(\n",
    "            example.get('qa_pairs')\n",
    "        )\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs_rag = eval_chain.evaluate(examples, predictions_rag, prediction_key=\"result\")\n",
    "print('The result of RAG evaluation for the given example questions: ')\n",
    "\n",
    "accuracy_rag = len([result for result in graded_outputs_rag if result['results'] == 'CORRECT'])/len(graded_outputs_rag)\n",
    "print(f'\\n Accuracy of the RAG pipeline: {accuracy_rag} \\n')\n",
    "\n",
    "graded_outputs_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of RAG pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_rag_large' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a dataframe\u001b[39;00m\n\u001b[32m      2\u001b[39m data = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mMethod\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mRAG\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHybrid RAG\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mTwo-Stage RAG\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mLarge Scale Accuracy (\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m: [\u001b[43maccuracy_rag_large\u001b[49m, accuracy_hybrid_large, accuracy_2RAG_large],\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSmall Scale Accuracy (\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m: [accuracy_rag_small, accuracy_hybrid_small, accuracy_2RAG_small]\n\u001b[32m      6\u001b[39m }\n\u001b[32m      7\u001b[39m df = pd.DataFrame(data)\n\u001b[32m      8\u001b[39m df_melted = df.melt(id_vars=\u001b[33m'\u001b[39m\u001b[33mMethod\u001b[39m\u001b[33m'\u001b[39m, var_name=\u001b[33m'\u001b[39m\u001b[33mExperiment Scale\u001b[39m\u001b[33m'\u001b[39m, value_name=\u001b[33m'\u001b[39m\u001b[33mAccuracy (\u001b[39m\u001b[33m%\u001b[39m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'accuracy_rag_large' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a dataframe\n",
    "data = {\n",
    "    'Method': ['RAG', 'Hybrid RAG', 'Two-Stage RAG'],\n",
    "    'Large Scale Accuracy (%)': [accuracy_rag_large, accuracy_hybrid_large, accuracy_2RAG_large],\n",
    "    'Small Scale Accuracy (%)': [accuracy_rag_small, accuracy_hybrid_small, accuracy_2RAG_small]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df_melted = df.melt(id_vars='Method', var_name='Experiment Scale', value_name='Accuracy (%)')\n",
    "df_melted['Accuracy (%)'] = (df_melted['Accuracy (%)']*100).round(2)\n",
    "df_melted.head()\n",
    "\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_plot = sns.barplot(\n",
    "    x='Method',\n",
    "    y='Accuracy (%)',\n",
    "    hue='Experiment Scale',\n",
    "    data=df_melted,\n",
    "    # palette='viridis'\n",
    ")\n",
    "\n",
    "# Add percentage labels on top of the bars\n",
    "for p in bar_plot.patches:\n",
    "    height = p.get_height()\n",
    "    bar_plot.annotate(f'{height}%',\n",
    "                      (p.get_x() + p.get_width() / 2., height),\n",
    "                      ha='center', va='bottom',\n",
    "                      fontsize=11)\n",
    "\n",
    "# Set labels and title\n",
    "plt.ylim(0, 100)  # Adjusted to fit percentage scale\n",
    "plt.title('Accuracy Comparison of RAG Methods Across Experiment Scales', fontsize=16)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Method')\n",
    "plt.legend(title='Experiment Scale')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Morningstar\n",
      "- Federal Reserve\n",
      "- Interest rate decisions\n",
      "- 2024\n",
      "- 2025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# lama-cpp-python test\n",
    "# 2. Load the model and generate a response\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# repo_model = \"hugging-quants/Llama-3.2-1B-Instruct-Q4_K_M-GGUF\"\n",
    "repo_model = \"hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF\"\n",
    "filename = \"*q4_k_m.gguf\"\n",
    "\n",
    "# Instantiate the Llama object (this loads the model into RAM)\n",
    "llm_cpp = Llama.from_pretrained(\n",
    "    repo_id=repo_model,\n",
    "    filename=filename,\n",
    "    local_dir=\"models\",\n",
    "    # n_ctx=5000,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "shortening_prompt = f\"\"\"\n",
    "You are an expert financial advisor tasked with shortening the original question. \n",
    "Your role is to reformulate the original question to short phrases with essential keywords.\n",
    "Mostly focus on company names, consultant or advisor names.\n",
    "The answer does not need to be complete sentense.\n",
    "Do not convert words to abbreviations.\n",
    "\n",
    "Original Question: \"{question}\"\n",
    "\n",
    "Reformulated phrases: \n",
    "\"\"\"\n",
    "# sample_question = \"Write a short, friendly greeting from a magical bookstore owner.\"\n",
    "response = llm_cpp.create_completion(\n",
    "    prompt=shortening_prompt,\n",
    "    max_tokens=128,\n",
    "    temperature=0,\n",
    "    # top_p=0.95,\n",
    "    stop=[\"\\n\\n\"]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 00:59:56,673 INFO [httpx] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morningstar view, Federal Reserve, interest rate decisions, 2024, 2025.\n"
     ]
    }
   ],
   "source": [
    "from backend.my_lib.LLMManager import LLMManager\n",
    "\n",
    "llm_manager = LLMManager(llm_instance = None)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert financial advisor tasked with shortening the original question. \n",
    "Your role is to reformulate the original question to short phrases with essential keywords.\n",
    "Mostly focus on company names, consultant or advisor names.\n",
    "The answer does not need to be complete sentense.\n",
    "Do not convert words to abbreviations.\n",
    "\n",
    "Original Question: \"{original_question}\"\n",
    "\n",
    "Reformulated phrases: \n",
    "\"\"\"\n",
    "invoke_kwargs = {\"original_question\": question}\n",
    "response = llm_manager.invoke(system_prompt, invoke_kwargs)\n",
    "print(response)\n",
    "# llm_manager.invoke(shortening_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Morningstar's view\n",
      "- Federal Reserve interest rate decisions\n",
      "- 2024 and 2025 timeframe\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# callback_manager = CallbackManager()\n",
    "\n",
    "model_path=\"models/llama-3.2-3b-instruct-q4_k_m.gguf\",\n",
    "# model_path=\"models/llama-3.2-1b-instruct-q4_k_m.gguf\",\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"models/llama-3.2-3b-instruct-q4_k_m.gguf\",\n",
    "    temperature=0,\n",
    "    max_tokens=128,\n",
    "    # top_p=1,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    stop=[\"\\n\\n\"]\n",
    ")\n",
    "\n",
    "question_test = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "response = llm.invoke(shortening_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 01:07:53,327 INFO [httpx] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-20 01:07:53.331 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 01:07:53,332 INFO [backend.my_lib.qa_chains] The shortened question:\n",
      " Morningstar view, Federal Reserve, interest rate decisions, 2024-2025.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morningstar view, Federal Reserve, interest rate decisions, 2024-2025.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 17.52it/s]\n",
      "2025-06-20 01:07:53.768 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 01:07:53,768 INFO [backend.my_lib.qa_chains] The small chunks were retrieved\n",
      "2025-06-20 01:07:53.769 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 01:07:53,769 INFO [backend.my_lib.qa_chains] The DRS was calculated for relevant PDF documents\n",
      "Batches: 100%|██████████| 2/2 [00:00<00:00, 13.59it/s]\n",
      "2025-06-20 01:07:53,985 INFO [backend.my_lib.retrievers] \n",
      " ==== Ranked retrieved large chunks ==== \n",
      "\n",
      "2025-06-20 01:07:53.986 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 01:07:53,986 INFO [backend.my_lib.qa_chains] The large chunks were retrieved\n",
      "2025-06-20 01:07:53.986 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 01:07:53,986 INFO [backend.my_lib.qa_chains] The aggregated scores were calculated for all retrieved chunks\n",
      "2025-06-20 01:07:53.987 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-06-20 01:07:53,987 INFO [backend.my_lib.qa_chains] The top score chunks were concatenated\n",
      "2025-06-20 01:07:57,175 INFO [httpx] HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Morningstar's view, as outlined in the documents, is that the Federal Reserve is expected to begin cutting the federal-funds rate starting at the September meeting of 2024. The base case projects that the federal-funds rate will end 2024 in the range of 4.75% to 5.00%. Furthermore, Morningstar forecasts that the Fed will continue to cut the federal-funds rate throughout 2025, with the rate expected to decrease to a range of 3.00% to 3.25% by the end of 2025. This outlook is based on a combination of moderating inflation and slowing economic growth.\n"
     ]
    }
   ],
   "source": [
    "from backend.my_lib.qa_chains import QAchains\n",
    "\n",
    "llm_manager = LLMManager(llm_instance = None)\n",
    "\n",
    "qa_chains = QAchains(retrievers, config, llm_manager = llm_manager)\n",
    "\n",
    "qa_chains.shorten_question(question)\n",
    "shortened_question = qa_chains.shortened_question\n",
    "print(shortened_question)\n",
    "qa_chains.retrieve_context()\n",
    "answer = qa_chains.generate_answer()\n",
    "print(answer)\n",
    "# return answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
