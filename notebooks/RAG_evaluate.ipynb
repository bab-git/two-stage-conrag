{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAG Pipelines for Document-Based Question Answering\n",
    "\n",
    "In this notebook, we assess the performance of three distinct Retrieval-Augmented Generation (RAG) pipelines—**Standard RAG**, **Two-Stage Consecutive RAG**, and **Hybrid RAG**—for document-based question answering tasks. \n",
    "Utilizing LangChain's `QAGenerateChain`, we generate a comprehensive set of question-answer pairs from a collection of PDF documents. Each RAG pipeline processes these questions to generate answers, which are then evaluated and scored using LangChain's evaluation tools. \n",
    "Finally, we compare the accuracy rates of each pipeline to determine their effectiveness in delivering precise and contextually relevant responses based on the provided documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bbkho\\AppData\\Local\\Temp\\ipykernel_55664\\3912488263.py:19: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../configs\", job_name=\"notebook_config\")\n",
      "d:\\Portfolio\\Evooq_rag\\.evqenv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-11-12 22:53:28.403 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "streamlit is not running\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Required imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.evaluation.qa import QAGenerateChain, QAEvalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "initialize(config_path=\"../configs\", job_name=\"notebook_config\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from helper_functions import PDFManager, Retrievers, QAchains, Hybrid_Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_root = \"../data/pdfs_selected/\"\n",
    "\n",
    "config = compose(config_name=\"config\")\n",
    "modelID = config.llm.openai_modelID\n",
    "top_k_BM25 = config.Retrieval.top_k_BM25\n",
    "top_k_semantic = config.Retrieval.top_k_semantic\n",
    "top_k_final = config.Retrieval.top_k_final\n",
    "\n",
    "question = \" According to the documents, what is Morningstar's view on the Federal Reserve's interest rate decisions for the remainder of 2024 and into 2025?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## document-level chunks\n",
    "Creating one chunk per pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f for f in glob.glob(data_root + '*.pdf') if os.path.isfile(f)]\n",
    "\n",
    "documents = []\n",
    "for file in filenames:    \n",
    "    loader = PyPDFLoader(f'{file}')\n",
    "    pages = loader.load()\n",
    "    # add all pages together    \n",
    "    combined_page_content = \"\".join([page.page_content for page in pages])\n",
    "    document = {\"page_content\": combined_page_content, \"metadata\": {\"source\": file}}\n",
    "    documents.append(document)\n",
    "    print(file)\n",
    "print(f'{len(documents)} PDF chunks created with one chunk per PDF\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA pair generation using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating n_samples of QA pairs from available PDFs\n",
    "n_samples = 10\n",
    "\n",
    "index = np.random.randint(1, len(documents), n_samples)\n",
    "sample_docs = [documents[i] for i in index]   \n",
    "llm = ChatOpenAI(temperature = 0.0, model=modelID)\n",
    "\n",
    "example_gen_chain = QAGenerateChain.from_llm(llm)\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in sample_docs],\n",
    ")\n",
    "print(f'{len(new_examples)} QA pairs generated from PDFs\\n')\n",
    "\n",
    "for i, example in enumerate(new_examples):\n",
    "    source = sample_docs[i]['metadata']['source']\n",
    "    print(\"\\n Source file:\\n\", source)    \n",
    "    example['source'] = source\n",
    "    print(f'Question:')\n",
    "    pprint(f'{example.get('qa_pairs').get('query')}')\n",
    "    print('Answer:')\n",
    "    pprint(f'{example.get('qa_pairs').get('answer')}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Stage RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "# config.settings.verbose = True\n",
    "\n",
    "pdf_manager = PDFManager(data_root, config)\n",
    "pdf_manager.load_pdfs()\n",
    "pdf_manager.chunk_documents()\n",
    "pdf_manager.create_vectorstore()\n",
    "\n",
    "retrievers = Retrievers(pdf_manager, config)\n",
    "retrievers.setup_retrievers()\n",
    "qa_chains = QAchains(retrievers, config)\n",
    "\n",
    "def two_stage_rag(question):\n",
    "    qa_chains.shorten_question(question)\n",
    "    qa_chains.retrieve_context()\n",
    "    answer = qa_chains.generate_answer()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA pair evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "examples = list()\n",
    "for example, idoc in zip(new_examples, index):\n",
    "    print(f'Document {idoc}')\n",
    "    question = example['qa_pairs']['query']\n",
    "    pprint(question)\n",
    "    answer = two_stage_rag(question)\n",
    "    prediction = {'query': example['qa_pairs']['query'], 'answer': example['qa_pairs']['answer']}\n",
    "    prediction[\"result\"] = answer\n",
    "    predictions.append(prediction)\n",
    "    examples.append(\n",
    "            example.get('qa_pairs')\n",
    "        )\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs_2RAG = eval_chain.evaluate(examples, predictions, prediction_key=\"result\")\n",
    "print('The result of RAG evaluation for the given example questions: ')\n",
    "\n",
    "accuracy_2RAG = len([result for result in graded_outputs_2RAG if result['results'] == 'CORRECT'])/len(graded_outputs_2RAG)\n",
    "print(f'\\n Accuracy of the RAG pipeline: {accuracy_2RAG} \\n')\n",
    "\n",
    "graded_outputs_2RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retrieval = Hybrid_Retrieval(pdf_manager, retrievers, config)\n",
    "hybrid_RAG_QA = QAchains(retrievers, config)\n",
    "def hybrid_rag(question,top_k_BM25, top_k_semantic, top_k_final, rrf_k = 60, hybrid = True):\n",
    "    top_score_docs = hybrid_retrieval.hybrid_retriever(question, top_k_BM25, top_k_semantic, top_k_final, rrf_k, hybrid)\n",
    "    hybrid_RAG_QA.top_score_docs = top_score_docs\n",
    "    hybrid_RAG_QA.question = question\n",
    "    answer = hybrid_RAG_QA.generate_answer()\n",
    "    return answer\n",
    "# answer_hybrid = hybrid_rag(question, 200, 50, 10)    \n",
    "# pprint(answer_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA pair evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_hybrid = list()\n",
    "examples = list()\n",
    "for example, idoc in zip(new_examples, index):\n",
    "    question = example['qa_pairs']['query']\n",
    "    print(f'Question:')\n",
    "    pprint(question)\n",
    "    answer = hybrid_rag(question, top_k_semantic, top_k_semantic, top_k_final)\n",
    "    print(f'Answer:')\n",
    "    pprint(answer)\n",
    "    prediction = {'query': example['qa_pairs']['query'], 'answer': example['qa_pairs']['answer']}\n",
    "    prediction[\"result\"] = answer\n",
    "    predictions_hybrid.append(prediction)\n",
    "    examples.append(\n",
    "            example.get('qa_pairs')\n",
    "        )\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs_hybrid = eval_chain.evaluate(examples, predictions_hybrid, prediction_key=\"result\")\n",
    "print('The result of RAG evaluation for the given example questions: ')\n",
    "\n",
    "accuracy_hybrid = len([result for result in graded_outputs_hybrid if result['results'] == 'CORRECT'])/len(graded_outputs_hybrid)\n",
    "print(f'\\n Accuracy of the RAG pipeline: {accuracy_hybrid} \\n')\n",
    "\n",
    "graded_outputs_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rag = list()\n",
    "examples = list()\n",
    "for example, idoc in zip(new_examples, index):\n",
    "    question = example['qa_pairs']['query']\n",
    "    print(f'Question:')\n",
    "    pprint(question)\n",
    "    answer = hybrid_rag(question, top_k_semantic, top_k_semantic, top_k_final, hybrid = False)\n",
    "    print(f'Answer:')\n",
    "    pprint(answer)\n",
    "    prediction = {'query': example['qa_pairs']['query'], 'answer': example['qa_pairs']['answer']}\n",
    "    prediction[\"result\"] = answer\n",
    "    predictions_rag.append(prediction)\n",
    "    examples.append(\n",
    "            example.get('qa_pairs')\n",
    "        )\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs_rag = eval_chain.evaluate(examples, predictions_rag, prediction_key=\"result\")\n",
    "print('The result of RAG evaluation for the given example questions: ')\n",
    "\n",
    "accuracy_rag = len([result for result in graded_outputs_rag if result['results'] == 'CORRECT'])/len(graded_outputs_rag)\n",
    "print(f'\\n Accuracy of the RAG pipeline: {accuracy_rag} \\n')\n",
    "\n",
    "graded_outputs_rag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of RAG pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "data = {\n",
    "    'Method': ['RAG', 'Hybrid RAG', 'Two-Stage RAG'],\n",
    "    'Large Scale Accuracy (%)': [accuracy_rag_large, accuracy_hybrid_large, accuracy_2RAG_large],\n",
    "    'Small Scale Accuracy (%)': [accuracy_rag_small, accuracy_hybrid_small, accuracy_2RAG_small]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df_melted = df.melt(id_vars='Method', var_name='Experiment Scale', value_name='Accuracy (%)')\n",
    "df_melted['Accuracy (%)'] = (df_melted['Accuracy (%)']*100).round(2)\n",
    "df_melted.head()\n",
    "\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create the bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_plot = sns.barplot(\n",
    "    x='Method',\n",
    "    y='Accuracy (%)',\n",
    "    hue='Experiment Scale',\n",
    "    data=df_melted,\n",
    "    # palette='viridis'\n",
    ")\n",
    "\n",
    "# Add percentage labels on top of the bars\n",
    "for p in bar_plot.patches:\n",
    "    height = p.get_height()\n",
    "    bar_plot.annotate(f'{height}%',\n",
    "                      (p.get_x() + p.get_width() / 2., height),\n",
    "                      ha='center', va='bottom',\n",
    "                      fontsize=11)\n",
    "\n",
    "# Set labels and title\n",
    "plt.ylim(0, 100)  # Adjusted to fit percentage scale\n",
    "plt.title('Accuracy Comparison of RAG Methods Across Experiment Scales', fontsize=16)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Method')\n",
    "plt.legend(title='Experiment Scale')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".evqenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
