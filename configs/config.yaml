# configs/config.yaml
settings:
  verbose: False

splitter:
  large_chunk_size: 2000
  small_chunk_size: 400 # chunk size for keyword search. small enough to capture a few sentenses
  paragraph_separator: "\n \n"

# LLMs Configuration
llm:
  openai_modelID : "gpt-4o-mini" #or gpt-4o
  # embed_model_id: 'sentence-transformers/all-mpnet-base-v2'
  embed_model_id: sentence-transformers/all-MiniLM-L12-v2 # embedding model for creating vector database for semantic search
  local_llama_model: "hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF"
  local_llama_filename: "*q4_k_m.gguf"

Vectorstore:  
  collection_name: large_chunks
  persist_directory : 'vector_store'
  clear_existing: False # Feature flag: to clear the vector store before rebuilding it at each run

Retrieval:
  semantic_CE_model : cross-encoder/stsb-TinyBERT-L-4 # cross encoder for similarity scoring of semantic search retults. The models are optimized for semantic similarity
  keyword_CE_model: cross-encoder/ms-marco-MiniLM-L-6-v2 # cross encoder for similarity scoring of BM25 keyword search results. MS MARCO Cross-Encoders are well-suited for tasks that involve keyword search
  top_k_BM25: 200 # top k results from BM25 keyword search
  top_k_documents: 5 # top relevant pdfs to select based on keyword search - np.ceil(top_k_BM25/40)
  top_k_semantic: 50 # top k chunk results from semantic search - top_k_documents*10
  top_k_final: 10 # Final selected k chunks from semantic search and BM25 keyword search to use for answer generation
