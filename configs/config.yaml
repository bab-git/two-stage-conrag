# configs/config.yaml
settings:
  verbose: False
  deployment_mode: ${DEPLOYMENT_MODE:local}  # Will read from env variable

# Model configurations for different deployment modes
models:
  local:
    openai:
      - name: "GPT-4o-mini"
        model_id: "gpt-4o-mini"
        provider: "openai"
        requires_key: true
      - name: "GPT-4o"
        model_id: "gpt-4o"
        provider: "openai"
        requires_key: true
    local_llama:
      - name: "LLaMA 3.2 3B (Local)"
        model_id: "hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF"
        filename: "*q4_k_m.gguf"
        provider: "llama_cpp"
        requires_key: false
  
  cloud:
    openai:
      - name: "GPT-4o-mini"
        model_id: "gpt-4o-mini"
        provider: "openai"
        requires_key: true
      - name: "GPT-4o"
        model_id: "gpt-4o"
        provider: "openai"
        requires_key: true
    groq:
      - name: "LLaMA 4 Scout 17B (30K Context)"
        model_id: "meta-llama/llama-4-scout-17b-16e-instruct"
        provider: "groq"
        requires_key: false    
      - name: "LLaMA 3.3 70B Versatile"
        model_id: "llama-3.3-70b-versatile"
        provider: "groq"
        requires_key: false
      - name: "Mistral Saba 24B"
        model_id: "mistral-saba-24b"
        provider: "groq"
        requires_key: false
      - name: "Gemma 2 9B IT"
        model_id: "gemma2-9b-it"
        provider: "groq"
        requires_key: false
      - name: "LLaMA 3.1 8B Instant"
        model_id: "llama-3.1-8b-instant"
        provider: "groq"
        requires_key: false


splitter:
  large_chunk_size: 2000
  small_chunk_size: 400 # chunk size for keyword search. small enough to capture a few sentenses
  paragraph_separator: "\n \n"

# LLMs Configuration
llm:
  openai_modelID : "gpt-4o-mini" #or gpt-4o
  # embed_model_id: 'sentence-transformers/all-mpnet-base-v2'
  embed_model_id: sentence-transformers/all-MiniLM-L12-v2 # embedding model for creating vector database for semantic search
  local_llama_model: "hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF"
  local_llama_filename: "*q4_k_m.gguf"

Vectorstore:  
  collection_name: large_chunks
  persist_directory : 'vector_store'
  clear_existing: False # Feature flag: to clear the vector store before rebuilding it at each run

Retrieval:
  semantic_CE_model : cross-encoder/stsb-TinyBERT-L-4 # cross encoder for similarity scoring of semantic search retults. The models are optimized for semantic similarity
  keyword_CE_model: cross-encoder/ms-marco-MiniLM-L-6-v2 # cross encoder for similarity scoring of BM25 keyword search results. MS MARCO Cross-Encoders are well-suited for tasks that involve keyword search
  top_k_BM25: 200 # top k results from BM25 keyword search
  top_k_documents: 5 # top relevant pdfs to select based on keyword search - np.ceil(top_k_BM25/40)
  top_k_semantic: 50 # top k chunk results from semantic search - top_k_documents*10
  top_k_final: 10 # Final selected k chunks from semantic search and BM25 keyword search to use for answer generation
